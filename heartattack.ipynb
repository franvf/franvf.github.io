{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndata = pd.read_csv('/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv')\nwarnings.filterwarnings('ignore') #Avoid print warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-14T14:33:49.287832Z","iopub.execute_input":"2021-06-14T14:33:49.288232Z","iopub.status.idle":"2021-06-14T14:33:49.306956Z","shell.execute_reply.started":"2021-06-14T14:33:49.288198Z","shell.execute_reply":"2021-06-14T14:33:49.305685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we are in front of a new data set the first thing to do is view the amount of data available, and see if we are in fornt of a classification or regression model.","metadata":{}},{"cell_type":"code","source":"print(data.shape)\nprint(data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:33:51.993506Z","iopub.execute_input":"2021-06-14T14:33:51.993856Z","iopub.status.idle":"2021-06-14T14:33:52.001566Z","shell.execute_reply.started":"2021-06-14T14:33:51.993825Z","shell.execute_reply":"2021-06-14T14:33:51.999271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see in the command output we have 303 rows and 14 columns, and seeing the column names the value to predict could be in the 'output' column. ","metadata":{}},{"cell_type":"code","source":"data['output'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:33:53.886612Z","iopub.execute_input":"2021-06-14T14:33:53.887056Z","iopub.status.idle":"2021-06-14T14:33:53.895236Z","shell.execute_reply.started":"2021-06-14T14:33:53.887015Z","shell.execute_reply":"2021-06-14T14:33:53.893553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we show the different values in the ouput column we can see that we only have two distinct values, the 0 and the 1. This tells to us that we are in front of a binary classification problem.","metadata":{}},{"cell_type":"markdown","source":"# **Exploratory data analysis. (EDA)**\n\nFor now we know what type of problem we are faced, but we don't know nothing about the data quality, for this reaso, the first job is explore the data to get information about that. A good starting point is getting some general information about our dataset.","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:33:56.685327Z","iopub.execute_input":"2021-06-14T14:33:56.685751Z","iopub.status.idle":"2021-06-14T14:33:56.705927Z","shell.execute_reply.started":"2021-06-14T14:33:56.685712Z","shell.execute_reply":"2021-06-14T14:33:56.70484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the code above we can see that we have 303 entries and 14 columns (information extracted from the first two lines) but we already known this information. In the table we see the different column names, if has or not null values and his data type, and with this information we can make some assumptions like this: \n- We don't have null values, then we don't have to fill this white spaces.\n- All columns are in a numeric type (integer and float), then we don't have to convert the content of these columns in to numeric type or modify the column type to numeric. ","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:33:58.961114Z","iopub.execute_input":"2021-06-14T14:33:58.961471Z","iopub.status.idle":"2021-06-14T14:33:58.978326Z","shell.execute_reply.started":"2021-06-14T14:33:58.961439Z","shell.execute_reply":"2021-06-14T14:33:58.977133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the table above we see the different columns and the content of the first five rows, and also we see that the range of the different values are different, e.g. the sex column contains values that are either 0 or 1 (depend on if the person is male or female) and the column named chol (cholesterol in mg/dL) contains high values, therefore we should standarize the data (modify the ranges of all the columns from -1 to 1) if we want to use algorithms that uses the distance to classify the data. If we would have different ranges in the columns the influence in the output of the columns will be different, this means that the model would take into account the columns with higher values to decide the final result. Before to standarize the model we should see if the columns have some correlation between them and decide if we should or not remove any column.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,6))\nsns.heatmap(data.corr(),cmap='coolwarm', annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:34:01.057372Z","iopub.execute_input":"2021-06-14T14:34:01.057728Z","iopub.status.idle":"2021-06-14T14:34:02.216837Z","shell.execute_reply.started":"2021-06-14T14:34:01.057698Z","shell.execute_reply":"2021-06-14T14:34:02.216025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seeing the correlation between columns we can say that all the columns are necessary, or the same, we don't should remove any column because the correlations are near to 0. If we had two columns with a correlation around 1 or -1 we should remove one of this columns, because a high correlation can lead us to get bad results. ","metadata":{}},{"cell_type":"markdown","source":"Other thing to do is see if we have duplicated values, the duplicated values can lead us to a bad perform.","metadata":{}},{"cell_type":"code","source":"data.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:34:05.354555Z","iopub.execute_input":"2021-06-14T14:34:05.354974Z","iopub.status.idle":"2021-06-14T14:34:05.365315Z","shell.execute_reply.started":"2021-06-14T14:34:05.354937Z","shell.execute_reply":"2021-06-14T14:34:05.36392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 1 duplicated value, this is not a problem because we can remove this value.","metadata":{}},{"cell_type":"code","source":"data = data.drop_duplicates()\ndata.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:34:07.170889Z","iopub.execute_input":"2021-06-14T14:34:07.171565Z","iopub.status.idle":"2021-06-14T14:34:07.187766Z","shell.execute_reply.started":"2021-06-14T14:34:07.171527Z","shell.execute_reply":"2021-06-14T14:34:07.186692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we don't have duplicated values, the next step is know if we have either balanced or unbalanced data, in case that we don't have a balanced dataset we could have problems to predict the label with few samples. To comprove this we have to count the different entries with output equals to 1 and entries with output equals to 0.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.countplot(data['output']) ","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:34:08.882078Z","iopub.execute_input":"2021-06-14T14:34:08.882522Z","iopub.status.idle":"2021-06-14T14:34:09.007155Z","shell.execute_reply.started":"2021-06-14T14:34:08.882479Z","shell.execute_reply":"2021-06-14T14:34:09.005661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have around 140 samples of no heart attack and around 165 samples of heart attack, this dataset it is balanced because have around 50% for each sample to predict, in case that the dataset contains 70-30 or more, the dataset should considered unbalanced, and in this case the treatment of the data would be dfferent.","metadata":{}},{"cell_type":"markdown","source":"Other interesting thing to do is see if we have balanced data but not for the final result, but for sex column. This is a good practice, because we could have problems predict the heart attack for the sex with few samples.","metadata":{}},{"cell_type":"code","source":"fig, ax3 = plt.subplots(figsize=(12,5))\ngrp2 = data.groupby(['sex', 'output'])['output'].count() #Rating mean\ngrp2.plot.bar(ax = ax3)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:34:12.20871Z","iopub.execute_input":"2021-06-14T14:34:12.209285Z","iopub.status.idle":"2021-06-14T14:34:12.359048Z","shell.execute_reply.started":"2021-06-14T14:34:12.209235Z","shell.execute_reply":"2021-06-14T14:34:12.358332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see in the table, we have more samples of the sex 1 than the sex 0, this could be a problem.I do not know if the conditions for having an heart attack are different in men and women, for this reason i go to see the feature importance, if the importance of sex is low, I'm will drop this column, but this step will take place after train the model. ","metadata":{}},{"cell_type":"markdown","source":"# Training\n\nNow that we have all the relevant information about our data set is time to make the necessary modifications and train the model. First of all we will select the independent variables (all the variables that helps us to predict the final result) and the dependent variable (variable to predict)","metadata":{}},{"cell_type":"code","source":"x = data.iloc[:, 0:13].values #Independent variables\ny = data.iloc[:, 13].values #Dependent variable","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:34:15.0228Z","iopub.execute_input":"2021-06-14T14:34:15.023442Z","iopub.status.idle":"2021-06-14T14:34:15.02978Z","shell.execute_reply.started":"2021-06-14T14:34:15.0234Z","shell.execute_reply":"2021-06-14T14:34:15.028463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data standarizaton\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx = sc.fit_transform(x)\nx[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:34:16.791935Z","iopub.execute_input":"2021-06-14T14:34:16.792342Z","iopub.status.idle":"2021-06-14T14:34:16.802722Z","shell.execute_reply.started":"2021-06-14T14:34:16.792296Z","shell.execute_reply":"2021-06-14T14:34:16.801468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data has been standarized, now we don't will have problems with the columns influence in the final result. As example we see the values for the different columns in the first row.","metadata":{}},{"cell_type":"markdown","source":"Now we need to split the data in a subset for train the model and a subset for test the performance of our model, for this reason we need to use the train_test_split function. Also we need to do this split for the independent variable and for the dependent variable.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T14:34:19.475183Z","iopub.execute_input":"2021-06-14T14:34:19.475574Z","iopub.status.idle":"2021-06-14T14:34:19.483507Z","shell.execute_reply.started":"2021-06-14T14:34:19.475542Z","shell.execute_reply":"2021-06-14T14:34:19.48192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to train the model, but for do it we have a lot of models, and a lot of hyperparameters. If we don't know what model can be fitting better we can use the grid search technique. This technique helps us to train the model with more than one classifier (or one if we want) and his hyperparaments with his different values for each one. ","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier \n\npipe = Pipeline(steps = [('estimator', RandomForestClassifier())])\n\nparams = [{\n    'estimator' : [SVC()],\n    'estimator__C' : [0, 0.15, 0.2, 1, 10], # Larger C avoids more miss-classifications\n    }, \n    {\n    'estimator' : [DecisionTreeClassifier()], # 0.813 train, 0.815 sin tau3, 0.81 test\n    'estimator__criterion' : ['gini', 'entropy'],\n    'estimator__max_depth' : range(0, 10),  #Maximum depth of the tree\n    'estimator__min_samples_split' : range(1,5), #Minimum of samples for split one node\n    'estimator__min_samples_leaf': range(1,5) #Minimum of samples to be a leaf node\n    }, \n    {\n    'estimator' : [XGBClassifier()],  #0.8712 train y test, 0.8604 sin tau3\n    'estimator__max_depth' : range(2,8), # range(4,8), #6 by default, then not remove\n    'estimator__learning_rate' : [0.11, 0.12, 0.13], \n    'estimator__n_estimators' : range(100,106),\n    'estimator__min_child_weight': range(1,4),\n    'estimator__min_split_loss' : range(1,4) # One node will be splited if the loss function is equal or less than the parameter value (put in 0)\n    },\n    {\n    'estimator' : [RandomForestClassifier()], #0.865 train, 0.866 sin tau3, 0.86 test\n    'estimator__n_estimators' : range(297, 302), #Number of trees in the forest\n    'estimator__max_depth' : range(3,6), \n    }]\n\nmodel_grid = GridSearchCV(pipe, cv=5, param_grid = params, verbose=2)\nmodel_grid.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:44:20.018414Z","iopub.execute_input":"2021-06-14T15:44:20.018825Z","iopub.status.idle":"2021-06-14T15:45:03.022977Z","shell.execute_reply.started":"2021-06-14T15:44:20.018789Z","shell.execute_reply":"2021-06-14T15:45:03.021573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_grid.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:47:48.168185Z","iopub.execute_input":"2021-06-14T15:47:48.168575Z","iopub.status.idle":"2021-06-14T15:47:48.176761Z","shell.execute_reply.started":"2021-06-14T15:47:48.168543Z","shell.execute_reply":"2021-06-14T15:47:48.175874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are trained different models, and the best model is Random Forest, the next step is evaluate the model with different metrics. The first metric that we should check is the accuracy, this metric evalueates the ability of the model for make a prediction. ","metadata":{}},{"cell_type":"code","source":"print(\"Accuracy train: {:.3f}\".format(model_grid.score(x_train, y_train))) #See the accuracy of training set\nprint(\"Accuracy test: {:.3f}\".format(model_grid.score(x_test, y_test))) #Print the accuracy of test set","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:47:55.099877Z","iopub.execute_input":"2021-06-14T15:47:55.100306Z","iopub.status.idle":"2021-06-14T15:47:55.181225Z","shell.execute_reply.started":"2021-06-14T15:47:55.100255Z","shell.execute_reply":"2021-06-14T15:47:55.179918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case we have an accuracy of 90% in the train set (the model is able to predict well the 90% of knowed samples) and is able to predict the 82'4% in the test set (data that the model has never seen) The accuracy are differents in both cases, this is quite normal but if we had a very high gap between the accuracies we have two cases: \n- The train model is very higher than the test model: In this case we talk about overffit, the model is able to do well predictions in date previously seen, but don't know how to generalize with data that never had seen.\n\n- The model performs very poorly also in train set, in this case we talk about underfit. \n\nIn our case we don't have any of this cases. ","metadata":{}},{"cell_type":"code","source":"y_pred = model_grid.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:52:25.016834Z","iopub.execute_input":"2021-06-14T15:52:25.017189Z","iopub.status.idle":"2021-06-14T15:52:25.058708Z","shell.execute_reply.started":"2021-06-14T15:52:25.01716Z","shell.execute_reply":"2021-06-14T15:52:25.05777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another metric to predict is the confusion matrix, the diagonal of the matrix represents the correct predictions, the other values are wrong predicted values. ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncnf = confusion_matrix(y_test, y_pred)\ncnf","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:13.065331Z","iopub.execute_input":"2021-06-14T15:35:13.065675Z","iopub.status.idle":"2021-06-14T15:35:13.072636Z","shell.execute_reply.started":"2021-06-14T15:35:13.065645Z","shell.execute_reply":"2021-06-14T15:35:13.071837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 34 negative cases labeled as negative cases, and 42 positives cases labeleds as positives, but then we have 11 cases labeleds as positives but actually are negatives, and 4 cases labeleds as negatives and are positives (in this cases this is a problem). Now let's analyze the ROC curve\n","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\nmetrics.plot_roc_curve(model_grid, x_test, y_test)  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T15:35:16.602482Z","iopub.execute_input":"2021-06-14T15:35:16.603126Z","iopub.status.idle":"2021-06-14T15:35:16.805139Z","shell.execute_reply.started":"2021-06-14T15:35:16.603077Z","shell.execute_reply":"2021-06-14T15:35:16.804359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the ROC curve we can obtain the proportion between true positives vs false positives, in our model we have a proportion of 0.92 or 92%","metadata":{}},{"cell_type":"markdown","source":"# Conclusions\n\nWe are trained a model to detect heart attacks, and we are achieve good results. This dataset are very easy to interpret because we don't have null values, unbalanced data, categorical variables and the amount of data is enough for do well predictions. In a real dataset we don't have this type of dataset and neither datasets with few columns with wich is easy to work. In real jobs we spend around the 80% of the time in EDA step, in our case we don't have spend to much time.\n\n","metadata":{}}]}